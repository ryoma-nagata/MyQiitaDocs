---
title: Azure Synapse Analytics へ Azure Data Factoryの資産を移行する
tags:
  - Microsoft
  - Azure
  - Spark
  - AzureDataFactory
  - AzureSynapseAnalytics
private: false
updated_at: '2020-12-23T08:50:20+09:00'
id: 1bbdab083c19fc6e9a53
organization_url_name: null
slide: false
---
# Azure Synapse Analytics へ Azure Data Factoryの資産を移行する

- [Azure Synapse Analytics へ Azure Data Factoryの資産を移行する](#azure-synapse-analytics-へ-azure-data-factoryの資産を移行する)
  - [はじめに](#はじめに)
  - [参考](#参考)
  - [移行イメージ](#移行イメージ)
  - [移行手順](#移行手順)
    - [利用資材](#利用資材)
    - [1. 新規Synapseを準備](#1-新規synapseを準備)
    - [2. 既存ADF環境のコード準備](#2-既存adf環境のコード準備)
    - [3. 既存ADF環境Git Repository切断](#3-既存adf環境git-repository切断)
    - [4. 新規Synapse環境Git Repository接続](#4-新規synapse環境git-repository接続)
    - [5. 参照先、アクティビティ修正](#5-参照先アクティビティ修正)
      - [Datasetの参照先修正](#datasetの参照先修正)
      - [アクティビティの参照先修正](#アクティビティの参照先修正)
    - [6. Databricks Notebook移行、修正(オプション)](#6-databricks -notebook移行修正オプション)
      - [Databricks Notebook移行](#databricks -notebook移行)
      - [Databricks Notebook内容修正](#databricks -notebook内容修正)
  - [補足](#補足)

## はじめに

GAおめでとうございます！
Git機能もついているということで、既存のAzure Data Factory (以下、ADF)のパイプライン移行手順を検証したいと思います。Databricks部分の移行も試してみます

今後、この移行はソリューションが提供されるかもしれませんが、いち早く移行したい方の参考になればと幸いです。

(2020年12月時点情報です)

## 参考

もうこれだけ見ればいいんじゃないかという良記事です。かなり参考にしてます。


[Azure Synapse Analytics 日本上陸記念! Synapse リソースの移行ステップ 2020年12月バージョン](https://qiita.com/dahatake/items/e976d70b32a4aa7d6be5)

## 移行イメージ

こんな計画をたてました。Databricksはオプションです。
※Databricksで使っているNotebookはそのまま使えない場合が多いので、事前検証しましょう

![mig.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/0f9da784-c551-f053-97d5-3c1d18585ac0.png)


## 移行手順

DataFactoryはそこそこ楽ですが、Databricksはかなり地道です。  
Databricks は[新しいワークスペース機能](https://databricks.com/jp/blog/2020/11/19/new-features-to-accelerate-the-path-to-production-with-the-next-generation-data-science-workspace.html)の、プロジェクト単位でのGit連携がきたら楽になるかもですね

1. 新規Synapseを準備
2. 既存ADF環境のコード準備
3. 既存ADF環境Git Repository切断
4. 新規Synapse環境Git Repository接続
5. 参照先、アクティビティ修正
6. Databricks Notebook移行、修正(オプション)


### 利用資材

[AzureCloudScaleAnalyticsHOL](https://github.com/ryoma-nagata/AzureCloudScaleAnalyticsHOL)で作成されるコード資産、Data FactoryとDatabricks部分を使ってみます。  
Githubのほうは閉域構成に対応させた内容にしていますが、この記事ではそのあたり無視して簡略化します。

このようなデータ移動→notebook実行のパイプラインと

![pipeline0.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/71f258af-f4af-4ee2-bf59-7e2fef7887c3.png)

いくつかのノートブックが対象です。 

![adbnotebook1.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/3bc29390-ea47-27ae-6d2c-c2aed54c198b.png)

※抜粋

SQLPool,Data Lake Storage gen2 のDataの移行に関しては参考記事をご確認ください。  
※その場合でもNW構成はあらためて設定が必要かと思います。

### 1. 新規Synapseを準備

[Synapse ワークスペースの作成](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/get-started-create-workspace)を実施します。

作成の際は既存のData lake Storage Gen2を選択して、

>[Data Lake Storage Gen2 アカウント 'xxx' のストレージ BLOB データ共同作成者ロールを自分に割り当てます。]

にチェックをつけます。
これで既存のストレージが流用できます。

### 2. 既存ADF環境のコード準備


もしDataFactoryのGit統合を設定していない場合は、[Git リポジトリに接続する](https://docs.microsoft.com/ja-jp/azure/data-factory/source-control#connect-to-a-git-repository)を実施してください。

Git環境にこのようなコードが保存されます。
self-hosted integration runtimeも利用している環境なのと、KeyVaultなんかもありますね。
このあたりは補足であつかっていきます。

DatalakeとSynapseへのLinked Serviceが移行対象です。


![jsons.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/df65cba4-8e65-ede0-0ce5-54d3ded943e1.png)


### 3. 既存ADF環境Git Repository切断

[別のGit リポジトリに切り替える](https://docs.microsoft.com/ja-jp/azure/data-factory/source-control#switch-to-a-different-git-repository)を実施します。

### 4. 新規Synapse環境Git Repository接続

[Synapse Studio で Git リポジトリを構成する](https://docs.microsoft.com/ja-jp/azure/synapse-analytics/cicd/source-control#configure-git-repository-in-synapse-studio)を実施します。

全手順で切断したGit Repositoryを利用します。

このような形でコードが追加されます。

![codes.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/b3f9f483-4293-09ff-2c1f-9f538d2c3904.png)


Synapse は初期状態では、Linked Serviceとして以下の二つと既定のAzure Integration Runtimeがコードで保存されています。

初期のLinked Serivice

- Synapseのリソース名-WorkspaceDefaultSqlServer
    - 専用SQLプールへの接続です。パラメータに専用SQLプール名を入力することで、各種機能から専用SQLプールへ接続が可能です。
- Synapseのリソース名-WorkspaceDefaultStorage
    - リソース作成時に指定したData Lake Storage


### 5. 参照先、アクティビティ修正

#### Datasetの参照先修正

地道な作業です。linked serviceのrelatedからリンクが張られているので、一つ一つ直していきます。
数が多い場合は一度localにcloneして、一括置換したほうがよいです。

対象はこの二つ
![linked edit target.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/13303bf2-6b38-c140-e975-e59a420bf495.png)



以下のように変更します。

Datalake用→Synapseのリソース名-WorkspaceDefaultStorage  に変更  

![update.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/42403e40-5644-ead6-09b5-c8acc836b627.png)




専用SQLプールSynapseのリソース名-WorkspaceDefaultSqlServerに変更

![dwupdate.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/22b17fd0-89de-0bc9-113e-51c70d8426ce.png)



結果
![update_linkedresulrt1.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/77eac662-5011-4afa-a8fe-176e5e18c14e.png)




不要なlinked Serviceは削除しておきましょう。

もし、sas トークンの認証をしていた場合など認証情報をDataFactoryに保持していた場合は設定しなおしてください。

#### アクティビティの参照先修正

databricksアクティビティをsynapseのnotebookアクティビティに変更して、実行するnotebookを選択します。

Synapse notebookに変更します。

![synapsenotebook.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/3e986bdd-d3a3-ff29-7d64-e4d0380a9054.png)


notebookの内容を選択しておきます。

![notebook_act2.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/d9c4d091-ed53-8991-cab4-76e8835c6093.png)


### 6. Databricks Notebook移行、修正(オプション)

#### Databricks Notebook移行

地道にipynb形式でノートブックをダウンロードします。

![adbnotebook1.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/a74700b5-0c59-a4c1-a6da-66adb5d19726.png)



Databricks CLIを利用すれば少し楽です

[ワークスペースフォルダーをローカルファイルシステムにエクスポートする](https://docs.microsoft.com/ja-jp/azure/databricks/dev-tools/cli/workspace-cli#export-a-workspace-folder-to-the-local-filesystem)

フォーマットオプションに関する参考：https://forums.databricks.com/questions/38399/databricks-cli-export-dir-to-save-ipynb-files-not.html

次に、Synapse Notebookとしてimportします。
複数ファイル選択してアップロード後、フォルダ構成などを修正します。

インポートから、すべてのファイルを一括アップしましょう
![imports.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/105c10e3-617d-b97d-6ea3-cfaea5dbdb9a.png)



フォルダ作成まで実施します。

![notebooksimported.png](https://qiita-image-store.s3.ap-northeast-1.amazonaws.com/0/281819/8ec97657-da94-7bd0-6e3b-a9688f15810b.png)


#### Databricks Notebook内容修正

参照しているパスは変えていきましょう。
マウントパスを指定している箇所をabfss:～ではじまるパスに書き換えます。

もし別のノートブックを呼び出しているパートがあったら、preview機能をオンにして、コードを書き換えます。

たとえば、

databricksでは「datalake」という名称のコンテナを「mnt/datalake」という名称でマウントしていた場合、

```

root_path = "dbfs:/mnt/datalake/contoso/"

```
と書いていましたが、

Synapseではこうなります。

```
# primary の場合
root_path = "/contoso/"　

```

delta テーブルはdatabricksでこういう形式での定義（列名を事前定義して、空のテーブルを作る）ができましたが、こちらはSynapseでは対応していない記法なので注意

```

spark.sql(
    """
    CREATE TABLE FactOnlineSales
      (
        OnlineSalesKey integer not null ,
～～～～～～～～～～～～～～～～～～～～～～～～～～～
      )
    USING delta
    LOCATION '{}'
    """.format(FactOnlineSales_delta_silver_path)\
)

```

Spark テーブルの挙動に関してはこちらの記事を参考にどうぞ  
[AzureSynapseAnalyticsのメタデータ共有についてわかったこと](https://qiita.com/ryoma-nagata/items/300ae6df431642bc9919#%E3%81%A1%E3%81%AA%E3%81%BF%E3%81%ABdelta-table%E3%82%92%E4%BD%9C%E6%88%90%E3%81%99%E3%82%8B%E3%81%A8)


## 補足

linked serviceに関して、以前のDatabricksアクティビティではData Factory->Databricksの認証設定が必要でしたが、Synapseでは不要となっています。  
認証の管理オーバヘッドが低減されるのはSynapse Analyticsの良さのひとつですね

